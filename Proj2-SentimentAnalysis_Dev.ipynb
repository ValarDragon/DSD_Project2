{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_chars = [chr(x) for x in range(31)]\n",
    "add_spaces = [\"[\", \"]\", \"(\", \")\", \"`\", \":\", \".\", \";\", \"{\", \"}\"]\n",
    "def segmentWords(s): \n",
    "    # Remove non-printable characters, change remove ending apostrophe or apostrophe s, as those\n",
    "    # make the dataset more sparse, and dont provide additional value regarding sentiment in the bag of words model.\n",
    "    # (Most of the time its just making an extra word for a proper noun)\n",
    "    for char in remove_chars:\n",
    "        s = s.replace(char,\"\")\n",
    "    for char in add_spaces:\n",
    "        s = s.replace(char,char + \" \")\n",
    "    words = [word if not word.endswith(\"'s\") else word[:-2] for word in s.split()]\n",
    "    return [word if not word.endswith(\"'\") else word[:-1] for word in words]\n",
    "\n",
    "    \n",
    "\n",
    "def readFile(fileName):\n",
    "    # Function for reading file\n",
    "    # input: filename as string\n",
    "    # output: contents of file as list containing single words\n",
    "    contents = []\n",
    "    f = open(fileName)\n",
    "    for line in f:\n",
    "        contents.append(line)\n",
    "    f.close()\n",
    "    result = segmentWords('\\n'.join(contents))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dataframe containing the counts of each word in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "document_frequency_for_words = {}\n",
    "\n",
    "for c in os.listdir(\"data_training\"):\n",
    "    directory = \"data_training/\" + c\n",
    "    for file in os.listdir(directory):\n",
    "        words = readFile(directory + \"/\" + file)\n",
    "        e = {x:words.count(x) for x in words}\n",
    "        for word in set(words):\n",
    "            if word not in document_frequency_for_words:\n",
    "                document_frequency_for_words[word] = 0\n",
    "            document_frequency_for_words[word] += 1\n",
    "        e['__FileID__'] = file\n",
    "        e['is_positive'] = 1 if c == 'pos' else 0\n",
    "        e['num_words'] = sum([1 for x in words if len(x) > 1]) #Exclude punctuation and empty strings\n",
    "        d.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from d - make sure to fill all the nan values with zeros.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d,index=None)\n",
    "if '' in df.columns:\n",
    "    df.drop('', inplace=True, axis=1)\n",
    "df.fillna(value=0,axis=0,inplace=True)\n",
    "df = df.astype(np.int32, errors='ignore') # Become slightly more memory efficient, because my computer is a potato.\n",
    "del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 30750 cols\n"
     ]
    }
   ],
   "source": [
    "min_document_frequency = 5\n",
    "bad_cols = []\n",
    "for word in document_frequency_for_words.keys():\n",
    "    if document_frequency_for_words[word] < min_document_frequency:\n",
    "        bad_cols += [word]\n",
    "print(\"Removing %s cols\" % len(bad_cols))\n",
    "df.drop(bad_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Sample 80% of your dataframe to be the training data\n",
    "\n",
    "* Let the remaining 20% be the validation data (you can filter out the indicies of the original dataframe that weren't selected for the training data)\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf, validatedf = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the dataframe for both training and validation data into x and y dataframes - where y contains the labels and x contains the words\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf_x = traindf.drop('is_positive', axis=1)\n",
    "traindf_x.drop('__FileID__', inplace=True, axis=1)\n",
    "traindf_y = pd.DataFrame(traindf['is_positive'])\n",
    "\n",
    "validatedf_x = validatedf.drop('is_positive', axis=1)\n",
    "validatedf_x.drop('__FileID__', inplace=True, axis=1)\n",
    "validatedf_y = pd.DataFrame(validatedf['is_positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Logistic Regression\n",
    "* Use sklearn's linear_model.LogisticRegression() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "* In the backward stepsize selection method, you can remove coefficients and the corresponding x columns, where the coefficient is more than a particular amount away from the mean - you can choose how far from the mean is reasonable.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.std.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you select which features to remove? Why did that reduce overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Decision Tree\n",
    "\n",
    "* Initialize your model as a decision tree with sklearn.\n",
    "* Fit the data and labels to the model.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree score:  0.653125\n"
     ]
    }
   ],
   "source": [
    "def most_important_features(dtc):\n",
    "    lst = []\n",
    "    for i in range(len(dtc.feature_importances_)):\n",
    "        if dtc.feature_importances_[i] > 0:\n",
    "            lst.append([dtc.feature_importances_[i], traindf_x.columns[i]])\n",
    "    lst.sort(key=lambda x: x[0], reverse=True)\n",
    "    return lst\n",
    "dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=7)\n",
    "dt_clf.fit(traindf_x, traindf_y)\n",
    "print(\"Decision tree score: \", dt_clf.score(validatedf_x, validatedf_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters\n",
    "* To test out which value is optimal for a particular parameter, you can either loop through various values or look into sklearn.model_selection.GridSearchCV\n",
    "\n",
    "References:\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest validation score is: 0.655208333333\n"
     ]
    }
   ],
   "source": [
    "train_scores = []\n",
    "valid_scores = []\n",
    "x = [x for x in range(1, 20)]\n",
    "num_trials = 3\n",
    "for depth in x:\n",
    "    training_score = 0\n",
    "    validation_score = 0\n",
    "    for i in range(num_trials):\n",
    "        clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth)\n",
    "        clf.fit(traindf_x, traindf_y)\n",
    "        training_score += clf.score(traindf_x, traindf_y)\n",
    "        validation_score += clf.score(validatedf_x, validatedf_y)\n",
    "    train_scores += [training_score / num_trials]\n",
    "    valid_scores += [validation_score / num_trials]\n",
    "print(\"Highest validation score is: \" + str(max(valid_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75da2d3160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, train_scores, color='b', label=\"Avg. Training Score\")\n",
    "plt.plot(x, valid_scores, color='r', label=\"Avg. Validation Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Question:_ How did you choose which parameters to change and what value to give to them? Feel free to show a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to change the maximum depth parameter. We chose to vary this parameter, as lowering the maximum depth helps prevent the decision tree from overfitting. Thus it seemed sensible to try to find the optimal max depth to increase the validation scores. I have determined the value to give it from the above plot. The plot I made is a graph of the average decision tree accuracy on the training data and test data, with 3 samples per depth. The above plot indicates that the maximum validation score occurs at a maximum depth of 7. (*Please note that the exact peak changes depending on the test data / validation data split. On average 7 appears to be the peak of the validation series on the plot I made, however this can change between run-throughs. Other common peaks occur between 6 and 8*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Question_ Why is a single decision tree so prone to overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree can train to fit the training data exactly, and thus accounts for details present within the training data that aren't representative of the true underlying relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an interesting sidenote, lets see which words are the most indicative of the data being positive / negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15250307514205746, 'bad'], [0.15214187150089234, 'and'], [0.040634127997180781, 'dull'], [0.034517869983163592, 'wasted'], [0.032251696476942628, 'worst'], [0.030570227749424087, 'terribly'], [0.029160119988766199, 'from'], [0.028311240121519029, 'been'], [0.027527721077483379, 'superb'], [0.027495236913870187, \"we'll\"]]\n"
     ]
    }
   ],
   "source": [
    "print(most_important_features(dt_clf)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try: #Visualize our tree, if graphviz is on sys\n",
    "    from sklearn.tree import export_graphviz\n",
    "    def visualize_tree(tree, feature_names):\n",
    "        import subprocess\n",
    "        \"\"\"Create tree png using graphviz.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        tree -- scikit-learn DecsisionTree.\n",
    "        feature_names -- list of feature names.\n",
    "        \"\"\"\n",
    "        with open(\"dt.dot\", 'w') as f:\n",
    "            export_graphviz(tree, out_file=f,\n",
    "                            feature_names=feature_names)\n",
    "        command = [\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"]\n",
    "        try:\n",
    "            subprocess.check_call(command)\n",
    "        except:\n",
    "            exit(\"Could not run dot, ie graphviz, to \"\n",
    "                 \"produce visualization. Run sudo-apt-get install graphviz to get it.\")\n",
    "    visualize_tree(dt_clf, traindf_x.columns)\n",
    "    del dt_clf\n",
    "except:\n",
    "    print(\"You probably need to install graphviz on your system if you want a visualization of the decision tree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree Visualization](dt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Random Forest\n",
    "\n",
    "* Use sklearn's ensemble.RandomForestClassifier() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest score:  0.83125\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=32, criterion='entropy')\n",
    "rf_clf.fit(traindf_x, traindf['is_positive'])\n",
    "print(\"Random forest score: \", rf_clf.score(validatedf_x, validatedf_y))\n",
    "del rf_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'max_depth': 32, 'criterion': 'entropy'}\n",
      "0.815625\n"
     ]
    }
   ],
   "source": [
    "parameters = {'criterion':['entropy'], 'n_estimators':[100], 'max_depth':[x for x in range(31,40)]}\n",
    "rf_clf = RandomForestClassifier()\n",
    "grd_src = GridSearchCV(rf_clf, parameters)\n",
    "grd_src.fit(traindf_x, traindf['is_positive'])\n",
    "print(grd_src.best_params_)\n",
    "print(grd_src.score(validatedf_x, validatedf_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Question_ What parameters did you choose to change and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We chose to change the number of estimators and the maximum depth. The number of estimators is the number of trees in the forest, so it seemed like that would be a good thing to find the optimal value for. The conclusion reached was the more estimators the better, however we are bounded by computational resources. We chose to vary the max depth because that would have a large impact on the accuracy of the model. (Maximum depth directly controls how much the model is overfitting / underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Question_: How does a random forest classifier prevent overfitting better than a single decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest trees help mitigate the problem of overfitting as they aggregate data into one final result. Random forest classifiers limit overfitting without substantially increasing error due to bias as we are able to set a max depth on the decision trees. Additionally, due to bagging, the features used in each individual tree are different, so when averaged, the result is improved. A way to (non-rigorously) intuit that random forests lower overfitting is that each individual decision tree comprising the random forest is still overfitting to the data. However, due to bagging, each decision tree will overfit in its own distinct way. Recall that bagging changes which entries are given to each decision tree, and which features the decision tree can split on. So each decision tree overfits to the data it was provided, and the ways its splits aren't common to every other tree. So when we average each of these distinct methods of overfitting on different components of our data, we end up with something that is actually representative of our data. This is because the methods that aren't very indicative of the entire data set (overfitting) won't appear in most decision trees, so their contribution will be surprised by the other trees in the forest. However methods that are helpful will strengthen the accuracy of the model. \n",
    "\n",
    "Thus random forests will not overfit our entire training set like single decision trees do. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
