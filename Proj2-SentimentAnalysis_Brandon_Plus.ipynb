{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_chars = [chr(x) for x in range(31)]\n",
    "add_spaces = [\"[\", \"]\", \"(\", \")\", \"`\", \":\", \".\", \";\", \"{\", \"}\"]\n",
    "def segmentWords(s): \n",
    "    # Remove non-printable characters, change remove ending apostrophe or apostrophe s, as those\n",
    "    # make the dataset more sparse, and dont provide additional value regarding sentiment in the bag of words model.\n",
    "    # (Most of the time its just making an extra word for a proper noun)\n",
    "    for char in remove_chars:\n",
    "        s = s.replace(char,\"\")\n",
    "    for char in add_spaces:\n",
    "        s = s.replace(char,char + \" \")\n",
    "    words = [word if not word.endswith(\"'s\") else word[:-2] for word in s.split()]\n",
    "    return [word if not word.endswith(\"'\") else word[:-1] for word in words]\n",
    "\n",
    "    \n",
    "\n",
    "def readFile(fileName):\n",
    "    # Function for reading file\n",
    "    # input: filename as string\n",
    "    # output: contents of file as list containing single words\n",
    "    contents = []\n",
    "    f = open(fileName)\n",
    "    for line in f:\n",
    "        contents.append(line)\n",
    "    f.close()\n",
    "    result = segmentWords('\\n'.join(contents))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dataframe containing the counts of each word in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "document_frequency_for_words = {}\n",
    "\n",
    "for c in os.listdir(\"data_training\"):\n",
    "    directory = \"data_training/\" + c\n",
    "    for file in os.listdir(directory):\n",
    "        words = readFile(directory + \"/\" + file)\n",
    "        e = {x:words.count(x) for x in words}\n",
    "        for word in set(words):\n",
    "            if word not in document_frequency_for_words:\n",
    "                document_frequency_for_words[word] = 0\n",
    "            document_frequency_for_words[word] += 1\n",
    "        e['__FileID__'] = file\n",
    "        e['is_positive'] = 1 if c == 'pos' else 0\n",
    "        e['num_words'] = sum([1 for x in words if len(x) > 1]) #Exclude punctuation and empty strings\n",
    "        d.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from d - make sure to fill all the nan values with zeros.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d,index=None)\n",
    "if '' in df.columns:\n",
    "    df.drop('', inplace=True, axis=1)\n",
    "df.fillna(value=0,axis=0,inplace=True)\n",
    "df = df.astype(np.int32, errors='ignore') # Become slightly more memory efficient, because my computer is a potato.\n",
    "del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 30750 cols\n"
     ]
    }
   ],
   "source": [
    "min_document_frequency = 5\n",
    "bad_cols = []\n",
    "for word in document_frequency_for_words.keys():\n",
    "    if document_frequency_for_words[word] < min_document_frequency:\n",
    "        bad_cols += [word]\n",
    "print(\"Removing %s cols\" % len(bad_cols))\n",
    "df.drop(bad_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_count = dict(((word,df[word].sum()) for word in  list(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Sample 80% of your dataframe to be the training data\n",
    "\n",
    "* Let the remaining 20% be the validation data (you can filter out the indicies of the original dataframe that weren't selected for the training data)\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf, validatedf = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the dataframe for both training and validation data into x and y dataframes - where y contains the labels and x contains the words\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf_x = traindf.drop('is_positive', axis=1)\n",
    "traindf_x.drop('__FileID__', inplace=True, axis=1)\n",
    "traindf_y = pd.DataFrame(traindf['is_positive'])\n",
    "\n",
    "validatedf_x = validatedf.drop('is_positive', axis=1)\n",
    "validatedf_x.drop('__FileID__', inplace=True, axis=1)\n",
    "validatedf_y = pd.DataFrame(validatedf['is_positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Logistic Regression\n",
    "* Use sklearn's linear_model.LogisticRegression() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "estimator = LogisticRegression()\n",
    "trained_model = estimator.fit(traindf_x,traindf_y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from basic model:  0.859375\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "# summarize the fit of the model\n",
    "accuracy_LR =  trained_model.score(validatedf_x, validatedf_y.values.ravel())\n",
    "print(\"Accuracy from basic model: \" ,accuracy_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any tuning of the hyper parameters and very little feature extraction/selection, the accuracy of the logistic regression model on the validation data is 85.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"Tuned Logistic Regression Parameters: {}\".format(trained_models.best_params_)) \n",
    "#print(\"Best score is {}\".format(trained_models.best_score_ ))\n",
    "#logReg_estimator = LogisticRegression(C = trained_models.best_params_['C'])\n",
    "logReg_estimator = LogisticRegression(C = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logReg_C =  logReg_estimator.fit(traindf_x,traindf_y.values.ravel())\n",
    "accuracy_LR_C = logReg_C.score(validatedf_x, validatedf_y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from tuned C parameter, model:  0.859375\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy from tuned C parameter, model: \" , accuracy_LR_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "* In the backward stepsize selection method, you can remove coefficients and the corresponding x columns, where the coefficient is more than a particular amount away from the mean - you can choose how far from the mean is reasonable.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.std.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "traindf_xNorm = preprocessing.normalize(traindf_x, norm=\"l2\", axis=1, copy=True, return_norm=False)\n",
    "normalizd_weights = preprocessing.normalize(logReg_C.coef_, norm =\"l2\", axis = 1, copy=True, return_norm=False)\n",
    "normalizd_weights.mean()\n",
    "keep_indices = []\n",
    "\n",
    "epsilon = 1500\n",
    "weights_mean = abs(normalizd_weights.mean())\n",
    "for i in range(len(normalizd_weights[0])):\n",
    "    if ((abs(abs(normalizd_weights[0][i] - weights_mean) / weights_mean)) < epsilon):\n",
    "        keep_indices.append(i)\n",
    "        \n",
    "traindf_xDropped = np.take(traindf_x,keep_indices, axis = 1)\n",
    "trained_logReg = logReg_estimator.fit(traindf_xDropped, traindf_y.values.ravel())\n",
    "validatedf_xDropped = np.take(validatedf_x, keep_indices, axis = 1)\n",
    "validatedf_xKept = preprocessing.normalize(validatedf_xDropped, norm=\"l2\", axis=1, copy=True, return_norm=False)\n",
    "trained_logReg.score(validatedf_xDropped,validatedf_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Parameter Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 11797 features.\n",
      "Fitting estimator with 11772 features.\n",
      "Fitting estimator with 11747 features.\n",
      "Fitting estimator with 11722 features.\n",
      "Fitting estimator with 11697 features.\n",
      "Fitting estimator with 11672 features.\n",
      "Fitting estimator with 11647 features.\n",
      "Fitting estimator with 11622 features.\n",
      "Fitting estimator with 11597 features.\n",
      "Fitting estimator with 11572 features.\n",
      "Fitting estimator with 11547 features.\n",
      "Fitting estimator with 11522 features.\n",
      "Fitting estimator with 11497 features.\n",
      "Fitting estimator with 11472 features.\n",
      "Fitting estimator with 11447 features.\n",
      "Fitting estimator with 11422 features.\n",
      "Fitting estimator with 11397 features.\n",
      "Fitting estimator with 11372 features.\n",
      "Fitting estimator with 11347 features.\n",
      "Fitting estimator with 11322 features.\n",
      "Fitting estimator with 11297 features.\n",
      "Fitting estimator with 11272 features.\n",
      "Fitting estimator with 11247 features.\n",
      "Fitting estimator with 11222 features.\n",
      "Fitting estimator with 11197 features.\n",
      "Fitting estimator with 11172 features.\n",
      "Fitting estimator with 11147 features.\n",
      "Fitting estimator with 11122 features.\n",
      "Fitting estimator with 11097 features.\n",
      "Fitting estimator with 11072 features.\n",
      "Fitting estimator with 11047 features.\n",
      "Fitting estimator with 11022 features.\n",
      "Fitting estimator with 10997 features.\n",
      "Fitting estimator with 10972 features.\n",
      "Fitting estimator with 10947 features.\n",
      "Fitting estimator with 10922 features.\n",
      "Fitting estimator with 10897 features.\n",
      "Fitting estimator with 10872 features.\n",
      "Fitting estimator with 10847 features.\n",
      "Fitting estimator with 10822 features.\n",
      "Fitting estimator with 10797 features.\n",
      "Fitting estimator with 10772 features.\n",
      "Fitting estimator with 10747 features.\n",
      "Fitting estimator with 10722 features.\n",
      "Fitting estimator with 10697 features.\n",
      "Fitting estimator with 10672 features.\n",
      "Fitting estimator with 10647 features.\n",
      "Fitting estimator with 10622 features.\n",
      "Fitting estimator with 10597 features.\n",
      "Fitting estimator with 10572 features.\n",
      "Fitting estimator with 10547 features.\n",
      "Fitting estimator with 10522 features.\n",
      "Fitting estimator with 10497 features.\n",
      "Fitting estimator with 10472 features.\n",
      "Fitting estimator with 10447 features.\n",
      "Fitting estimator with 10422 features.\n",
      "Fitting estimator with 10397 features.\n",
      "Fitting estimator with 10372 features.\n",
      "Fitting estimator with 10347 features.\n",
      "Fitting estimator with 10322 features.\n",
      "Fitting estimator with 10297 features.\n",
      "Fitting estimator with 10272 features.\n",
      "Fitting estimator with 10247 features.\n",
      "Fitting estimator with 10222 features.\n",
      "Fitting estimator with 10197 features.\n",
      "Fitting estimator with 10172 features.\n",
      "Fitting estimator with 10147 features.\n",
      "Fitting estimator with 10122 features.\n",
      "Fitting estimator with 10097 features.\n",
      "Fitting estimator with 10072 features.\n",
      "Fitting estimator with 10047 features.\n",
      "Fitting estimator with 10022 features.\n",
      "Fitting estimator with 9997 features.\n",
      "Fitting estimator with 9972 features.\n",
      "Fitting estimator with 9947 features.\n",
      "Fitting estimator with 9922 features.\n",
      "Fitting estimator with 9897 features.\n",
      "Fitting estimator with 9872 features.\n",
      "Fitting estimator with 9847 features.\n",
      "Fitting estimator with 9822 features.\n",
      "Fitting estimator with 9797 features.\n",
      "Fitting estimator with 9772 features.\n",
      "Fitting estimator with 9747 features.\n",
      "Fitting estimator with 9722 features.\n",
      "Fitting estimator with 9697 features.\n",
      "Fitting estimator with 9672 features.\n",
      "Fitting estimator with 9647 features.\n",
      "Fitting estimator with 9622 features.\n",
      "Fitting estimator with 9597 features.\n",
      "Fitting estimator with 9572 features.\n",
      "Fitting estimator with 9547 features.\n",
      "Fitting estimator with 9522 features.\n",
      "Fitting estimator with 9497 features.\n",
      "Fitting estimator with 9472 features.\n",
      "Fitting estimator with 9447 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "logReg_model = LogisticRegression()\n",
    "rfe = RFE(logReg_model, step = 25, n_features_to_select = int(.8*traindf_x.shape[1] // 1), verbose = 1)\n",
    "trained = rfe.fit(traindf_x, traindf_y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.859375"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validatedf_x.shape\n",
    "trained.score(validatedf_x, validatedf_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_space = np.logspace(start = -2, stop = 3, base = 2, num =3)\n",
    "iter_space = np.logspace(start = 2, stop = 4, base = 10, num = 3)\n",
    "penalty_space = [\"l2\", \"l1\"]\n",
    "tol_space = np.logspace(start = -6, stop= -3, base = 10, num = 3)\n",
    "intercept_space = [True, False]\n",
    "\n",
    "param_grid = {'C': c_space, 'tol' : tol_space, 'fit_intercept' : intercept_space, 'max_iter' : iter_space, 'penalty' : penalty_space}\n",
    "\n",
    "logReg_model = LogisticRegression()\n",
    "\n",
    "logReg_CV = GridSearchCV(estimator = logReg_model, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 3)\n",
    "trained_models = logReg_CV.fit(traindf_x, traindf_y.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you select which features to remove? Why did that reduce overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Decision Tree\n",
    "\n",
    "* Initialize your model as a decision tree with sklearn.\n",
    "* Fit the data and labels to the model.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree score:  0.678125\n"
     ]
    }
   ],
   "source": [
    "def most_important_features(dtc):\n",
    "    lst = []\n",
    "    for i in range(len(dtc.feature_importances_)):\n",
    "        if dtc.feature_importances_[i] > 0:\n",
    "            lst.append([dtc.feature_importances_[i], df.columns[i]])\n",
    "    lst.sort(key=lambda x: x[0], reverse=True)\n",
    "    return lst\n",
    "dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=9)\n",
    "dt_clf.fit(traindf_x, traindf_y)\n",
    "print(\"Decision tree score: \", dt_clf.score(validatedf_x, validatedf_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters\n",
    "* To test out which value is optimal for a particular parameter, you can either loop through various values or look into sklearn.model_selection.GridSearchCV\n",
    "\n",
    "References:\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest validation score is: 0.668125\n"
     ]
    }
   ],
   "source": [
    "train_scores = []\n",
    "valid_scores = []\n",
    "x = [x for x in range(1, 100,5)]\n",
    "num_trials = 5\n",
    "for depth in x:\n",
    "    training_score = 0\n",
    "    validation_score = 0\n",
    "    for i in range(num_trials):\n",
    "        clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth)\n",
    "        clf.fit(traindf_x, traindf_y)\n",
    "        training_score += clf.score(traindf_x, traindf_y)\n",
    "        validation_score += clf.score(validatedf_x, validatedf_y)\n",
    "    train_scores += [training_score / num_trials]\n",
    "    valid_scores += [validation_score / num_trials]\n",
    "print(\"Highest validation score is: \" + str(max(valid_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VIXV//HPIYgRQVlVZBfZCZuRVZRdqj6gogLWFq2K\nfaz6qz/FrVQtvnxqq621Su0PK9KFsjzUWp4+qCQgFVwJoCJLABEhSBXZZJEleH5/3JlkEgKZJJPM\nZOb7fr3mNXPXOXO5OXM598655u6IiEhqqBHvAEREpOoo6YuIpBAlfRGRFKKkLyKSQpT0RURSiJK+\niEgKUdIXEUkhSvoiIilESV9EJIXUjHcAxTVq1MhbtWoV7zBERKqV5cuXf+XujUubL+GSfqtWrcjJ\nyYl3GCIi1YqZfRbNfCrviIikECV9EZEUoqQvIpJClPRFRFKIkr6ISAopNemb2TQz+9LMPj7BdDOz\n35rZRjP7yMx6Rkwbb2YbQo/xsQxcRETKLpoj/enAiJNM/w7QNvSYADwPYGYNgEeA3kAv4BEzq1+R\nYEVEpGJKvU7f3d80s1YnmWUU8CcP7rv4rpnVM7MmwEAgy913AZhZFsGXx8yKBi1Skm+/ha+/hl27\nCh+7dxe+Pnw43hGKnFyzZjBhQuW+Ryx+nNUU2BoxnBcad6LxxzGzCQT/S6BFixYxCEmqs6NHiybr\nkhJ4SdN27w4S/4mYVd1nECmP3r2rR9KvMHefCkwFyMzM1J3ak8jhw5CbW7YEvn//iddnBvXqQYMG\nhY82baB+/aLjwo/w+Pr14dRTq+5ziySqWCT9bUDziOFmoXHbCEo8keMXx+D9pBo4ehSmTYPJk+Hz\nz4+ffsopRZNz8+bQrVvRRF3S44wzIC2t6j+PSLKIRdKfB9xhZrMITtrudfftZvY68F8RJ2+HAw/G\n4P0kgR07BrNmwcMPw6ZN0K8fPPkkNGlSNHnXrq1yi0g8lJr0zWwmwRF7IzPLI7gi5xQAd/89MB+4\nDNgIHARuCk3bZWaPActCq5ocPqkryccd/vEP+OlP4eOPg6P2f/4TLrtMyV0kkURz9c64UqY78KMT\nTJsGTCtfaFJdZGfDT34C778P7drB7NlwzTVQQz/9E0k4+rOUcnv3XRgyBIYNg+3b4cUXYfVquO46\nJXyRRKU/TSmzjz6CUaOgb19YtQqeeQbWr4cf/ABqJsT1YCJyIvoTlaht2ACPPBKcqD3jDHj8cbjr\nLqhTJ96RiUi0lPSlVFu3wmOPBZdgnnoqPPAATJwYXFopItWLkr6c0I4d8POfw+9+F/zS9fbb4aGH\n4Jxz4h2ZiJSXkr4cZ+9e+NWv4Omn4eBBGD8+uO5e96sXqf6U9KXAwYPw3HPwxBNBm4Rrrw1+Uduh\nQ7wjE5FY0dU7wpEjQQmnTRu4/37o0weWL4c5c5TwRZKNjvRT2LFjMGNGcEXO5s1w0UVBoh8wIN6R\niUhl0ZF+CnKHl1+GjIygXl+/Prz6Krz5phK+SLJT0k8h7rBgAVx4IYweHQz/939DTg6MGKEeOSKp\nQEk/Rbz1FgwaBJdeCl99BdOnB7+mVY8ckdSiP/ckt3IlXH55UK9fty64Oic3NyjrqGWCSOrRn32S\n+uKLoEXCnDlBzf6JJ+COO+D00+MdmYjEk5J+EnKH7343KOlMmgT33BPcYlBEREk/CU2dCgsXwu9/\nD7fdFu9oRCSRqKafZDZvhnvvhaFDYcKEeEcjIokmqqRvZiPMLNfMNprZAyVMb2lmC83sIzNbbGbN\nIqYdM7MPQo95sQxeinKHW24JXv/hD7oEU0SOF809ctOAKcAwIA9YZmbz3H1NxGxPAX9y9z+a2WDg\n58D3QtO+cffuMY5bShBZ1mnZMt7RiEgiiuZIvxew0d03ufsRYBYwqtg8nYBFoddvlDBdKpnKOiIS\njWiSflNga8RwXmhcpA+Bq0OvrwLqmlnD0HC6meWY2btmdmVJb2BmE0Lz5OzYsaMM4QuorCMi0YvV\nidx7gUvMbCVwCbANOBaa1tLdM4Hrgd+YWZviC7v7VHfPdPfMxo0bxyik1BEu6zz1lMo6InJy0Vyy\nuQ1oHjHcLDSugLt/TuhI38zqAKPdfU9o2rbQ8yYzWwz0AD6pcOQCqKwjImUTzZH+MqCtmbU2s1rA\nWKDIVThm1sjMwut6EJgWGl/fzE4NzwP0ByJPAEsFqKwjImVVatJ393zgDuB1YC0wx91Xm9lkMxsZ\nmm0gkGtm64GzgcdD4zsCOWb2IcEJ3ieKXfUjFaCyjoiUlbl7vGMoIjMz03NycuIdRsLbvDnoh9+n\nT9AuWUf5IqnNzJaHzp+elH6RWw2prCMi5aXeO9WQfoQlIuWlI/1q5rPPdLWOiJSfkn41orKOiFSU\nyjvVyNSpkJ2tso6IlJ+O9KsJlXVEJBaU9KsBlXVEJFZU3qkGVNYRkVjRkX6CC5d1hgxRWUdEKk5J\nP4FFlnVefFFlHRGpOJV3EtgLL6isIyKxpSP9BPXZZ3DPPSrriEhsKeknIJV1RKSyqLyTgFTWEZHK\noiP9BKOyjohUJiX9BKIfYYlIZVN5J4GEyzrPPw+tWsU7GhFJRlEd6ZvZCDPLNbONZvZACdNbmtlC\nM/vIzBabWbOIaePNbEPoMT6WwSeTyLLObbfFOxoRSValJn0zSwOmAN8BOgHjzKxTsdmeAv7k7l2B\nycDPQ8s2AB4BegO9gEfMrH7swk8OKuuISFWJ5ki/F7DR3Te5+xFgFjCq2DydgEWh129ETL8UyHL3\nXe6+G8gCRlQ87OQSLus8+aTKOiJSuaJJ+k2BrRHDeaFxkT4Erg69vgqoa2YNo1wWM5tgZjlmlrNj\nx45oY08KKuuISFWK1dU79wKXmNlK4BJgG3As2oXdfaq7Z7p7ZuPGjWMUUuJTWUdEqlo0V+9sA5pH\nDDcLjSvg7p8TOtI3szrAaHffY2bbgIHFll1cgXiTiq7WEZGqFs2R/jKgrZm1NrNawFhgXuQMZtbI\nzMLrehCYFnr9OjDczOqHTuAOD41LeeGyzuDBKuuISNUpNem7ez5wB0GyXgvMcffVZjbZzEaGZhsI\n5JrZeuBs4PHQsruAxwi+OJYBk0PjUpp664hIvJi7xzuGIjIzMz0nJyfeYVSqqVODo/vnn4cf/jDe\n0YhIMjCz5e6eWdp8asNQxVTWEZF4UtKvQu5w663Ba5V1RCQe1HunCr3wAmRl6WodEYkfHelXEZV1\nRCQRKOlXAZV1RCRRqLxTBf7wB5V1RCQx6Ei/km3ZUljW0Z2wRCTelPQrUfhHWO5BWaeGtraIxJnK\nO5VIZR0RSTQ69qwkKuuISCJS0q8EKuuISKJSeacSqKwjIolKx6Axtn27yjoikriU9GPs5Zdh3z54\n9lmVdUQk8SgtxVh2dlDS6dgx3pGIiBxPST+G8vPhjTdg2DC1WhCRxKSkH0PLl8PevTB0aLwjEREp\nWVRJ38xGmFmumW00swdKmN7CzN4ws5Vm9pGZXRYa38rMvjGzD0KP38f6AySSrKzgefDg+MYhInIi\npV6yaWZpwBRgGJAHLDOzee6+JmK2SQT3zn3ezDoB84FWoWmfuHv32IadmLKzoUcPaNQo3pGIiJQs\nmiP9XsBGd9/k7keAWcCoYvM4cEbo9ZnA57ELsXo4cADeflulHRFJbNEk/abA1ojhvNC4SI8CN5hZ\nHsFR/p0R01qHyj7/MrMBJb2BmU0wsxwzy9mxY0f00SeQJUvg6NHgJK6ISKKK1YncccB0d28GXAb8\n2cxqANuBFu7eA/i/wF/N7IziC7v7VHfPdPfMxo0bxyikqpWVBaeeChddFO9IREROLJqkvw1oHjHc\nLDQu0s3AHAB3fwdIBxq5+2F33xkavxz4BGhX0aATUXY29O8Pp50W70hERE4smqS/DGhrZq3NrBYw\nFphXbJ4twBAAM+tIkPR3mFnj0IlgzOw8oC2wKVbBJ4ovvoCPPlI9X0QSX6lX77h7vpndAbwOpAHT\n3H21mU0Gctx9HnAP8IKZ3U1wUvdGd3czuxiYbGZHgW+BH7r7rkr7NHGyaFHwrHq+iCS6qLpsuvt8\nghO0keMejni9BuhfwnJ/A/5WwRgTXnY21K8fXK4pUpKjR4+Sl5fHoUOH4h2KVHPp6ek0a9aMU045\npVzLq7VyBbkHJ3EHD4a0tHhHI4kqLy+PunXr0qpVK0w9OqSc3J2dO3eSl5dH69aty7UOtWGooA0b\nYOtW1fPl5A4dOkTDhg2V8KVCzIyGDRtW6H+MSvoVlJ0dPKueL6VRwpdYqOh+pKRfQeFWyuedF+9I\nREr3yiuvYGasW7cu5ut+/PHH6d69O927dyctLa3g9W9/+9uo1/Hee+9x9913n3SeY8eOMWBAib/z\nLLP9+/czduxYMjIy6NKlCwMGDODgwYMxWXeiMnePdwxFZGZmek5OTrzDiEp+ftBn59pr4YUX4h2N\nJLK1a9fSMQFusjBmzBg+//xzBg8ezM9+9rNKe586deqwf//+Eqfl5+dTs2ZinE587LHH2LdvH7/8\n5S8BWLduHW3atCn3SVKoms9X0v5kZsvdPbO0ZXWkXwFqpSzVyf79+1m6dCkvvvgis2bNKhg/duxY\n/vd//7dg+MYbb2Tu3LkcPHiQ6667jk6dOnHVVVfRu3dvyntAdsMNN/Cf//mf9OrVi4ceeoh3332X\nvn370qNHD/r378+GDRsAyM7O5sorrwRg0qRJ3HzzzVxyySWcd955TJkyBQiSar169QrmHzJkCFdf\nfTXt27fn+9//fsF7zps3j/bt23PBBRdw5513Fqw30vbt22natLCrTIcOHQoS/ksvvUTXrl3p1q0b\nN910EwCffvopgwYNomvXrgwbNoy8vLwSP9/+/fu58cYb6dWrFz169OB//ud/yrXdKkNifN1WU+F6\nvlopS1n8+MfwwQexXWf37vCb35x8nn/84x+MGDGCdu3a0bBhQ5YvX84FF1zAmDFjmDNnDpdffjlH\njhxh4cKFPP/880yZMoX69euzZs0aPv74Y7p3r1iz3O3bt/Puu+9So0YN9u7dy5IlS6hZsyavvfYa\nkyZNYvbs2ccts379ehYuXMiePXvo2LEjP/zhD4+bZ8WKFaxevZqzzz6bPn368O6779K1a1duv/12\n3nrrLVq0aMF1111XYkw333wzI0aMYPbs2QwZMoTx48dz/vnn8+GHH/KLX/yCt99+mwYNGrBrV/Dz\nottvv51bbrmF7373u0ydOpUf//jHzJ0797jPd9999zFixAimT5/O7t276d27N8OGDSM9Pb1C2zAW\ndKRfAeFWytW0XZCkmJkzZzJ27FggOLqfOXMmAN/5znd44403OHz4MK+++ioXX3wxp512GkuXLi2Y\nv0uXLnTt2rVC73/ttddSI3Tj6D179jB69Gi6dOnCvffey+rVq0tc5oorrqBWrVqcddZZNGjQgJIa\nMvbp04dzzz234DzC5s2bWbNmDe3bt6dly5aYGePGjStx/RdccAGbNm3innvu4auvviIzM5P169ez\naNEixowZQ4MGDQAKnt97772CbfL973+fJUuWlPj5FixYUHCOY9CgQRw6dIgtW7aUc8vFlo70y+nA\nAXjrreCoTaQsSjsirwy7du1i0aJFrFq1CjPj2LFjmBlPPvkk6enpDBw4kNdff53Zs2cXJLVYO/30\n0wte/+QnP+HSSy/l9ttvZ+PGjYwYMaLEZU499dSC12lpaeTn55drnpOpW7cuo0ePZvTo0bg7r776\napmWD4v8fO7OK6+8Qps2bcq1rsqkI/1yCrdSVj1fqoO5c+fyve99j88++4zNmzezdetWWrduXXCk\nOmbMGF566SWWLFlSkID79+/PnDlzAFizZg2rVq2KWTx79+4tqKVPnz49ZusN69SpE7m5uWzduhV3\nL7F0BLB06VL27NkDwOHDh1m7di0tW7Zk8ODBzJ49u6CsE37u06dPwTb5y1/+wsUXX1ziei+99FKe\nffbZguGVK1fG7LNVlJJ+OWVnQ61aaqUs1cPMmTO56qqriowbPXp0QYln+PDh/Otf/2Lo0KHUqlUL\nCOrXO3bsoFOnTkyaNInOnTtz5plnAnDLLbeU+6QuwP3338/EiRPp2bMnlXEFYe3atXnuuecYOnQo\nmZmZ1KtXryD2SBs2bGDAgAFkZGTQs2dP+vbty6hRo+jWrRv33XcfF198Md27d2fixIkATJkyhalT\np9K1a1dmz57N008/XeL7P/LIIxw4cICMjAw6d+7Mo48+GvPPWF66ZLOcuneHhg1h4cJ4RyLVQaJc\nslkWx44d4+jRo6Snp/PJJ58wdOhQcnNzC74UEt3+/fupU6cO7s5tt91GRkYGd955Z+kLVgMVuWRT\nNf1y+PJL+PBD+K//inckIpXn4MGDDBo0iKNHj+Lu/O53v6s2CR/g+eefZ8aMGRw+fJjMzExuvfXW\neIeUEJT0yyF8dK96viSzunXrVqiEE28TJ04sKMtIIdX0yyHcSrlnz3hHIiJSNkr6ZaRWyiJSnUWV\n9M1shJnlmtlGM3ughOktzOwNM1tpZh+Z2WUR0x4MLZdrZpfGMvh42LhRrZRFpPoqtaYfusftFGAY\nkAcsM7N5obtlhU0C5rj782bWieAuW61Cr8cCnYFzgWwza+fux2L9QapKVlbwrKQvItVRNEf6vYCN\n7r7J3Y8As4BRxeZx4IzQ6zOBz0OvRwGz3P2wu38KbAytr9rKzoaWLSEBf2gnUqrKbK38r3/9i759\n+xYZl5+fz9lnn83nn39+gqXg0Ucf5amnngLg4YcfJjvc1CrC4sWLueKKK076/h988AHz5xfe1XXe\nvHk88cQTZfkIJ/T444/TuXNnunbtSvfu3Xnvvfdist54iCbpNwW2RgznhcZFehS4wczyCI7ywxfD\nRrNstXHsWHAT9KFDQffDkOpo5syZXHTRRQU/yoqlAQMGkJeXx2effVYwLjs7m86dO3PuuedGtY7J\nkycztJz/jS6e9EeOHMkDDxxXjS6zd955h3/+85+sWLGCjz76iOzsbJo3b16hdZa1VUQsxepE7jhg\nurs3Ay4D/mxmUa/bzCaYWY6Z5ZTUUClRhFsp6y5ZUh1VdmvlGjVqcN111xVZ96xZswqanb3wwgtc\neOGFdOvWjdGjR5d4s5LwewO89tprdOjQgZ49e/Lyyy8XzPP+++8XtGXu168fubm5HDlyhIcffpjZ\ns2fTvXt3Zs+ezfTp07njjjsA2Lx5M4MHD6Zr164MGTKkoPnZjTfeyF133UW/fv0477zzCt470vbt\n22nUqFFBj59GjRoVfIktW7aMfv360a1bN3r16sW+ffs4dOgQN910ExkZGfTo0YM33ngDCNpNjBw5\nksGDBzNkyBAAnnzySS688EK6du3KI488AsCBAwe4/PLL6datG126dDlhC4nyiuY6/W1A5Ndas9C4\nSDcDIwDc/R0zSwcaRbks7j4VmArBL3KjDb6qhev5aqUsFRKn3spV0Vp53Lhx3Hrrrdx///0cPnyY\n+fPn8+tf/xqAq6++uuAHUpMmTeLFF1884S9kDx06xK233sqiRYs4//zzGTNmTMG0Dh06FLRlzs7O\n5qGHHuJvf/sbkydPJicnh+eeew4o2tPnzjvvZPz48YwfP55p06Zx11138corrwBBUl+6dCnr1q1j\n5MiRXHPNNUViGT58OJMnT6Zdu3YMHTqUMWPGcMkll3DkyBHGjBnD7NmzufDCC/n666857bTTeOaZ\nZzAzVq1axbp16xg+fDjr168HKPjfQoMGDViwYAEbNmzg/fffx90ZOXIkb775Jjt27ODcc88t+CLe\nu3dvqdu9LKI5Gl8GtDWz1mZWi+DE7Lxi82wBhgCYWUcgHdgRmm+smZ1qZq2BtsD7sQq+qmVnB39b\naqUs1VFVtFbOzMxk//795Obm8uqrr9K7d++CtsQff/xxQZ+bGTNmnLCdMgR3sGrdujVt27bFzLjh\nhhsKpu3du5drr72WLl26cPfdd590PWHvvPMO119/PQDf+973WLp0acG0K6+8kho1atCpUye++OKL\n45atU6cOy5cvZ+rUqTRu3JgxY8Ywffp0cnNzadKkCRdeeCEAZ5xxBjVr1mTp0qUF8Xbo0IGWLVsW\nJP1hw4YVbI8FCxawYMECevToQc+ePVm3bh0bNmwgIyODrKws7r//fpYsWVJiz6CKKPVI393zzewO\n4HUgDZjm7qvNbDKQ4+7zgHuAF8zsboKTujd60NRntZnNAdYA+cCPquuVOwcOwNtvw113xTsSqfbi\n0Fu5Klsrjxs3jlmzZrF27doifexvvPFGXnnlFbp168b06dNZvHhxudb/05/+lEGDBvH3v/+dzZs3\nM3DgwArFG9ma+US9yNLS0hg4cCADBw4kIyODP/7xj1xwwQVlfq/i7ZcffPBBbrvttuPmW7FiBfPn\nz2fSpEkMGTKEhx9+uMzvdSJR1d3dfb67t3P3Nu7+eGjcw6GEj7uvcff+7t7N3bu7+4KIZR8PLdfe\n3cvXqDoBLF0KR46oni/VU1W2Vh43bhx/+ctfWLRoEaNGFV7ot2/fPpo0acLRo0eZMWPGSdfRoUMH\nNm/ezCeffAJQ5MTzidoy161bl3379pW4vn79+hWca5gxY0aZbqyem5tbcDtHCE4Yt2zZkvbt27N9\n+3aWLVtW8Pny8/MZMGBAwedbv349W7ZsoX379set99JLL2XatGkF9xLetm0bX375JZ9//jm1a9fm\nhhtuYOLEiaxYsSLqWKOhX+RGSa2UpTqrytbKHTt25PTTT2fw4MFFjmwfe+wxevfuTf/+/enQocNJ\n401PT2fq1Klcfvnl9OzZk7POOqtg2n333ceDDz5Ijx49ilwFM2jQINasWVNwIjfSs88+W3DP2z//\n+c8888wzpW2yAvv372f8+PF06tSJrl27smbNGh599FFq1arF7NmzufPOO+nWrRvDhg3j0KFD3H77\n7Xz77bdkZGQUlIIi/zcRNnz4cK6//nr69u1LRkYG11xzDfv27WPVqlX06tWL7t2787Of/YxJkyZF\nHWs01Fo5St27Q4MGwSWbImWl1soSS2qtXMnCrZQffzzekYhUnereWllKpqQfhfDRver5kkqqe2tl\nKZlq+lHIzoZ69dRKWUSqPyX9UqiVssRKop0/k+qpovuRkn4pNm6ELVvUVVMqJj09nZ07dyrxS4W4\nOzt37iQ9Pb3c61BNvxThhn9K+lIRzZo1Iy8vj0TuLSXVQ3p6Os2aNSv38kr6pQi3Uj7//HhHItXZ\nKaecQuvWreMdhojKOyejVsoikmyU9E9i+XLYs0elHRFJHkr6JxGu56uVsogkCyX9kwi3Uo5o+yEi\nUq0p6Z/AwYPw1lsq7YhIclHSP4ElS4JWykr6IpJMlPRPINxKuQxtt0VEEp6S/glkZ0P//lC7drwj\nERGJnaiSvpmNMLNcM9toZg+UMP1pM/sg9FhvZnsiph2LmFb83roJaceO4L7VKu2ISLIp9Re5ZpYG\nTAGGAXnAMjOb5+5rwvO4+90R898J9IhYxTfu3j12IVe+hQuDZyV9EUk20Rzp9wI2uvsmdz8CzAJG\nnWT+ccDMk0xPeNnZcOaZUI77HouIJLRokn5TYGvEcF5o3HHMrCXQGoi8qWC6meWY2btmdmW5I60i\naqUsIsks1idyxwJz3f1YxLiWofs2Xg/8xszaFF/IzCaEvhhy4t2F8JNPglbKukuWiCSjaJL+NqB5\nxHCz0LiSjKVYacfdt4WeNwGLKVrvD88z1d0z3T2zcePGUYRUebKygmfV80UkGUWT9JcBbc2stZnV\nIkjsx12FY2YdgPrAOxHj6pvZqaHXjYD+wJriyyaS7Gxo0UKtlEUkOZWa9N09H7gDeB1YC8xx99Vm\nNtnMRkbMOhaY5UVvDdQRyDGzD4E3gCcir/pJNGqlLCLJLqqbqLj7fGB+sXEPFxt+tITl3gYyKhBf\nlVqxImilrHq+iCQr/SI3glopi0iyU9KPkJUF3bqplbKIJC8l/RC1UhaRVKCkH7J0adBKWfV8EUlm\nSvoh4VbKF10U70hERCqPkn5IVhb06wennx7vSEREKo+SPmqlLCKpQ0mf4AdZoKQvIslPSZ/CVsqZ\nmfGORESkcqV80lcrZRFJJSmf9D/5BD77TKUdEUkNKZ/0w60XlPRFJBUo6YdaKbdtG+9IREQqX0on\nfbVSFpFUk9JJf8UK2L1bpR0RSR0pnfTD9fwhQ+Ibh4hIVUn5pK9WyiKSSqJK+mY2wsxyzWyjmT1Q\nwvSnzeyD0GO9me2JmDbezDaEHuNjGXxFHDwYdNZUaUdEUkmpt0s0szRgCjAMyAOWmdm8yHvduvvd\nEfPfCfQIvW4APAJkAg4sDy27O6afohzCrZSV9EUklURzpN8L2Ojum9z9CDALGHWS+ccBM0OvLwWy\n3H1XKNFnASMqEnCshFspDxgQ70hERKpONEm/KbA1YjgvNO44ZtYSaA0sKuuyVS07W62URST1xPpE\n7lhgrrsfK8tCZjbBzHLMLGfHjh0xDul4X30FK1eqtCMiqSeapL8NaB4x3Cw0riRjKSztRL2su091\n90x3z2zcuHEUIVXMwoXBs5K+iKSaaJL+MqCtmbU2s1oEiX1e8ZnMrANQH3gnYvTrwHAzq29m9YHh\noXFxFW6lfMEF8Y5ERKRqlXr1jrvnm9kdBMk6DZjm7qvNbDKQ4+7hL4CxwCx394hld5nZYwRfHACT\n3X1XbD9C2YRbKQ8aBDVL/fQiIsklqrTn7vOB+cXGPVxs+NETLDsNmFbO+GJu06aglfJ998U7EhGR\nqpdyv8jNygqeVc8XkVSUckk/OxuaN1crZRFJTSmV9NVKWURSXUol/ZUrg1bKw4bFOxIRkfhIqaQf\nbqU8eHB84xARiZeUSvpZWdC1K5x9drwjERGJj5RJ+mqlLCKSQkn/rbeCVsqq54tIKkuZpJ+dDaec\nolbKIpLaUibpZ2WplbKISEokfbVSloRx4EDwKGxRJVKlUqLl2KLQLV2U9KXKHDoE69bBxx8XPlat\ngi1bguk1a8IZZwTtXsPP4Ufk8Mmm1a0LaWnx/ZxS7aRE0g+3Us7MjHckknSOHYNPPilM6uEEv2FD\nMA2Ck0nqrqNxAAALZUlEQVQdO8JFF0GnTsHw3r3B4+uvC19v3QqrVxcO5+eX/v516kT3BVF8Wr16\n0LKlWs2moKT/F1crZYkJd8jLK3rU/vHHsHZtcFQPQW+PNm2gSxe49trguUuXoNHTKaeU/f2++abo\nl0LxL4mShnfvhs2bC4e/+ebE71GrVvBlFI4zIyN4btFCfUqq2pEj8MUXcPgwnH9+pb5V0qfBTZuC\nv4F77413JFJt7NxZ9Kg9/Ni7t3Cepk2DBDl4cGHC7NgRateOTQxmwbpq14Zzzin/eo4eLfrFEH69\nc2dQflq1Ct58E2bMKFymbl3o3LnwSyD8OOusin+uVOIebO9//xu2bw8e4dfFn3fuDJbp0wfeeefk\n662gpE/64dYLqufLcfbvhzVrji/N/PvfhfPUqxckv+uvL0yCnTtDgwbxi7ssTjkFGjYMHiezd29Q\nWorcDi+/DC+8UDjPWWcV/RLIyAi2Rd26lfsZEs2xY/DllyUn7+LjSvqf1qmnBl/kTZoER/UDBhQO\nt2lT6eGnRNJv3hzatYt3JFJl3IP/Lu/fD/v2FT5v3ly0PPPpp4XLnHZakMBGjCia1Jo0SY1Sx5ln\nBtc09+tXOM49KDkUPxn94ovBFUhhLVsWLQ916QIdOgTJrTo5cKD0JL59O+zYAd9+e/zy9esXJu9+\n/QpfF3+uVy+u+1RUSd/MRgDPENwu8Q/u/kQJ81wHPAo48KG7Xx8afwxYFZpti7uPjEHcUQm3Uh41\nKjX+bquto0eDxBx+hBP1yV6XNt+JToKmpUH79nDhhfCDHxQmqdatdSVMcWZBojrnnKL/Vf722+D2\nc8X/h7RgQfBvCcG2bNfu+P8ZnHde7LZzfn7F9pHI13v2BM/F1awZNOtq0gSaNQv2m+JJvEmTYJ70\n9Nh8rkpWatI3szRgCjAMyAOWmdk8d18TMU9b4EGgv7vvNrPI4t837t49xnFHZeVK2LVLpZ1KcexY\n8EcSzYnGr78++R/e4cPRv2/t2kE5oU6dwkeDBsHJxzp1ik4rPl/TpkHCr25HoImmRo3gS7J1a/iP\n/ygcf+RIcNVS5P8KVqyAuXMLf5eQnh5cwRT+EmjTJvj3L8+XfFn3m+L7Rf36hfvNmWeWfFTesGHw\neZNINEf6vYCN7r4JwMxmAaOANRHz3ApMcffdAO7+ZawDLY9wPX/IkPjGkXCOHCnbFSElDZd0VFRc\nWlrh5YLhP7S6deHcc0tOzMWTdPFptWvraDyR1aoVlMg6d4YxYwrHHzgQXOUU+b+C7Gz4059KXk96\nesn7wDnnnHh/Odm+c/rp2m8iRJP0mwJbI4bzgN7F5mkHYGZvEZSAHnX310LT0s0sB8gHnnD3VyoW\ncvSys6uolfKBA8EZ98WLg0dOTnAkXKNGsLNFPpc0rrTn8s777beFCTsycUdzhHTaacdf333uuSe/\nFrz48Gmnqa4mQdLNzDz+hzI7dwZlosij8NNPL/vlrVImsTqRWxNoCwwEmgFvmlmGu+8BWrr7NjM7\nD1hkZqvc/ZPIhc1sAjABoEWLFjEJ6JtvglbKP/pRTFZX1IED8PbbhUn+/feD+mJaWlDzu/32IOF9\n+22Q/MvyXJZ58/ODBH6iecyCJHzWWcG14qUl6fDwGWcER20ilSmaq4ok5qJJ+tuA5hHDzULjIuUB\n77n7UeBTM1tP8CWwzN23Abj7JjNbDPQAiiR9d58KTAXIzMyMSVOSpUuDfBiTen5pSf7ee2HgwOCM\nfapdviYi1Uo0SX8Z0NbMWhMk+7HA9cXmeQUYB7xkZo0Iyj2bzKw+cNDdD4fG9wd+GbPoT6JCrZRP\nlORr1gyS/MSJhUm+Tp3YBi4iUolKTfrunm9mdwCvE9Trp7n7ajObDOS4+7zQtOFmtgY4Bkx0951m\n1g/4f2b2LUFHzycir/qpTNnZ0LdvlDl5//6iSX7ZMiV5EUlK5gnW4jUzM9NzcnIqtI6vvgrK2JMn\nw6RJJcxQWpIfOFBJXkSqFTNb7u6ltpVMyl/kLloUXBZcUM8/WZLv1Qvuu68wyesuKyKSxJIy6S95\ndT9X1X6LXn9fDHcvDi6hVJIXEUmipP/VV/CrX8HixTz97jJqcgx+rSQvIhIpeZJ+rVrwzDMc6tST\np3iAjDsGMuqJvkryIiIRkifpn3EG7NrFn/6Uzk9vg3V3AMr3IiJFJFcnofR0srKCZnhqpSwicryk\nSvrhVspDh6rli4hISZIq6X/wQdBKediweEciIpKYkirpq5WyiMjJJVXSz8oK7stQ6a2URUSqqaRJ\n+uFWyrpLlojIiSVN0t+zB66+Gq64It6RiIgkrqS5Tr9JE/jrX+MdhYhIYkuaI30RESmdkr6ISApR\n0hcRSSFK+iIiKSSqpG9mI8ws18w2mtkDJ5jnOjNbY2arzeyvEePHm9mG0GN8rAIXEZGyK/XqHTNL\nA6YAw4A8YJmZzYu8162ZtQUeBPq7+24zOys0vgHwCJAJOLA8tOzu2H8UEREpTTRH+r2Aje6+yd2P\nALOAUcXmuRWYEk7m7v5laPylQJa77wpNywJGxCZ0EREpq2iSflNga8RwXmhcpHZAOzN7y8zeNbMR\nZVhWRESqSKx+nFUTaAsMBJoBb5pZRrQLm9kEYEJocL+Z5ZbhvRsBX5Vh/mSn7VGUtkchbYuikm17\ntIxmpmiS/jagecRws9C4SHnAe+5+FPjUzNYTfAlsI/giiFx2cfE3cPepwNRoAi7OzHLcPbM8yyYj\nbY+itD0KaVsUlarbI5ryzjKgrZm1NrNawFhgXrF5XiGU3M2sEUG5ZxPwOjDczOqbWX1geGiciIjE\nQalH+u6eb2Z3ECTrNGCau682s8lAjrvPozC5rwGOARPdfSeAmT1G8MUBMNndd1XGBxERkdKZu8c7\nhgoxswmh8pCg7VGctkchbYuiUnV7VPukLyIi0VMbBhGRFFKtk3407SGSlZk1N7M3Ilpf/J/Q+AZm\nlhVqe5EVOoGeMswszcxWmtk/Q8Otzey90D4yO3QxQkows3pmNtfM1pnZWjPrm6r7h5ndHfo7+djM\nZppZeqruG9U26Ue0h/gO0AkYZ2ad4htVlcoH7nH3TkAf4Eehz/8AsNDd2wILQ8Op5P8AayOGfwE8\n7e7nA7uBm+MSVXw8A7zm7h2AbgTbJeX2DzNrCtwFZLp7F4ILUsaSovtGtU36RNceImm5+3Z3XxF6\nvY/gD7opwTb4Y2i2PwJXxifCqmdmzYDLgT+Ehg0YDMwNzZIy28PMzgQuBl4EcPcj7r6H1N0/agKn\nmVlNoDawnRTdN6pz0leLhxAzawX0AN4Dznb37aFJ/wbOjlNY8fAb4D7g29BwQ2CPu+eHhlNpH2kN\n7ABeCpW7/mBmp5OC+4e7bwOeArYQJPu9wHJSdN+ozklfADOrA/wN+LG7fx05zYNLs1Li8iwzuwL4\n0t2XxzuWBFET6Ak87+49gAMUK+Wkyv4ROm8xiuCL8FzgdFK48WN1TvrRtIdIamZ2CkHCn+HuL4dG\nf2FmTULTmwBfnmj5JNMfGGlmmwlKfYMJatr1Qv+lh9TaR/KAPHd/LzQ8l+BLIBX3j6HAp+6+I9Qq\n5mWC/SUl943qnPSjaQ+RtEL16heBte7+64hJ84DwzWrGA/+o6tjiwd0fdPdm7t6KYF9Y5O7fBd4A\nrgnNlkrb49/AVjNrHxo1BFhDau4fW4A+ZlY79HcT3hYpuW9U6x9nmdllBHXccHuIx+McUpUxs4uA\nJcAqCmvYDxHU9ecALYDPgOtSrfWFmQ0E7nX3K8zsPIIj/wbASuAGdz8cz/iqipl1JzipXYugF9ZN\nBAd6Kbd/mNnPgDEEV72tBG4hqOGn3L5RrZO+iIiUTXUu74iISBkp6YuIpBAlfRGRFKKkLyKSQpT0\nRURSiJK+iEgKUdIXEUkhSvoiIink/wOJ6BcVahZ/xQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2de937acc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, train_scores, color='b', label=\"Avg. Training Score\")\n",
    "plt.plot(x, valid_scores, color='r', label=\"Avg. Validation Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you choose which parameters to change and what value to give to them? Feel free to show a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to change the maximum depth parameter. I chose to vary this parameter, as lowering the maximum depth helps prevent the decision tree from overfitting. Thus it seemed sensible to try to find the optimal max depth to increase the validation scores. I have determined the value to give it from the above plot. The plot I made is a graph of the average decision tree accuracy on the training data and test data, with 5 samples per depth. The above plot indicates that the maximum validation score occurs at a maximum depth of 9. (*Please note that the exact peak changes depending on the test data / validation data split. On average 9 appears to be the peak of the validation series on the plot I made, however this can change between run-throughs. Other common peaks (that are typically shorter than the peaks at 9) occur at 5 and 16*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a single decision tree so prone to overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree can train to fit the training data exactly, and thus accounts for details present within the training data that aren't representative of the true underlying relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an interesting sidenote, lets see which words are the most indicative of the data being positive / negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12354839186131988, 'bacteria'], [0.1005960852127845, 'ancy'], [0.039550223190104483, 'hirt'], [0.037641220924938074, 'worshippers'], [0.028163497132233858, 'allowing'], [0.025108318435031883, 'ridiculing'], [0.02281490948516933, 'nothiiiiiinggggggggggg'], [0.020946552654621722, 'shaping'], [0.019923462945768803, '?'], [0.019611698506784458, 'placards']]\n"
     ]
    }
   ],
   "source": [
    "print(most_important_features(dt_clf)[:10])\n",
    "del dt_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words really don't look they should be indicative of movie reviews in general. I wouldn't expect bacteria, or ancy to appear in the vast majority of reviews. This explains why the score on the validation data is so low (~70%). I suspect that there is some bias in the amount of reviews in the training set where it makes sense to have the words bacteria, and ancy. When you do multiple run-throughs with different validation / training splits, bacteria and ancy are consistenly in the top 10, indicating that there may indeed be some inherent bias in the given data. This could likely be fixed by getting more movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Random Forest\n",
    "\n",
    "* Use sklearn's ensemble.RandomForestClassifier() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest score:  0.76875\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=50, max_depth=30, criterion='entropy')\n",
    "rf_clf.fit(traindf_x, traindf['is_positive'])\n",
    "print(\"Random forest score: \", rf_clf.score(validatedf_x, validatedf_y))\n",
    "del rf_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'criterion':['entropy'], 'n_estimators':[40, 45, 50, 55, 60, 100], 'max_depth':[25,30,35,40]}\n",
    "rf_clf = RandomForestClassifier()\n",
    "grd_src = GridSearchCV(rf_clf, parameters)\n",
    "grd_src.fit(traindf_x, traindf['is_positive'])\n",
    "print(grd_src.best_params_)\n",
    "print(grd_src.score(validatedf_x, validatedf_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters did you choose to change and why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We chose to change the number of estimators and the maximum depth. The number of estimators is the number of trees in the forest, so it seemed like that would be a good thing to fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a random forest classifier prevent overfitting better than a single decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
